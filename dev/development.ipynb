{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customfunctions\n",
    "\n",
    "> Custom Functions POC Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will become your README and also the index of your documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/jdemlow/github/customfunctions\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: customfunctions\n",
      "  Building wheel for customfunctions (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for customfunctions: filename=customfunctions-0.0.1-py3-none-any.whl size=13669 sha256=719ccb0e4c96cf0fb8835158a09dcd2ad091d5bc29c4e3f86891172f7a7356b6\n",
      "  Stored in directory: /private/var/folders/hm/zsqyytm950g1dc_00qtbp2zh0000gn/T/pip-ephem-wheel-cache-ybp0e5v6/wheels/f5/9b/14/f9f412895d657c5fe5ab82c86c9b78cd8b2a4d167d7cdb8def\n",
      "Successfully built customfunctions\n",
      "Installing collected packages: customfunctions\n",
      "Successfully installed customfunctions-0.0.1\n"
     ]
    }
   ],
   "source": [
    "! cd ../ && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from customfunctions.connection import SnowflakeConnection\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, sum, avg, count, to_char, current_timestamp\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "# Suppress the specific Pydantic warning about schema\n",
    "warnings.filterwarnings(\"ignore\", message=\"Field name \\\"schema\\\" .* shadows an attribute in parent \\\"BaseModel\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active session with get_active_session() moving to create_snowflake_session. Error Seen:\n",
      "(1403): No default Session is found. Please create a session before you call function 'udf' or use decorator '@udf'.\n",
      "\n",
      "Connection Established with the following parameters:\n",
      "User                        : JD_SERVICE_ACCOUNT_ADMIN\n",
      "Role                        : \"DATA_SCIENTIST\"\n",
      "Database                    : \"DATASCIENCE\"\n",
      "Schema                      : \"CUSTOM_FUNCTIONS\"\n",
      "Warehouse                   : \"DS_WH_XS\"\n",
      "Snowflake version           : 8.46.1\n",
      "Snowpark for Python version : 1.26.0\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "\n",
    "    # We can also use Snowpark for our analyses!\n",
    "    from snowflake.snowpark.context import get_active_session\n",
    "    session = get_active_session()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"No active session with get_active_session() moving to create_snowflake_session. Error Seen:\\n{e}\")\n",
    "    config = {\n",
    "        'user': os.getenv('SNOWFLAKE_USER', ''),\n",
    "        'password': os.getenv('SNOWFLAKE_PASSWORD', ''),\n",
    "        'account': os.getenv('SNOWFLAKE_ACCOUNT', ''),\n",
    "        'database': 'DATASCIENCE',\n",
    "        'warehouse': 'DS_WH_XS',\n",
    "        'schema': 'CUSTOM_FUNCTIONS',\n",
    "        'role': 'DATA_SCIENTIST'\n",
    "    }\n",
    "    sfc = SnowflakeConnection(**config)\n",
    "    session = sfc.get_session()\n",
    "    snowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\n",
    "    snowpark_version = VERSION\n",
    "    print('\\nConnection Established with the following parameters:')\n",
    "    print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "    print('Role                        : {}'.format(session.get_current_role()))\n",
    "    print('Database                    : {}'.format(session.get_current_database()))\n",
    "    print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "    print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "    print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "    print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_data(\n",
    "    session,\n",
    "    num_rows: int = 10000,\n",
    "    table_name: str = \"ORDERS\",\n",
    "    seed: Optional[int] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create and load fake order data into Snowflake using lists.\n",
    "    \"\"\"\n",
    "    # Set random seed if provided\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    try:\n",
    "        # Create data as lists\n",
    "        data = [\n",
    "            [\n",
    "                i,  # ORDER_ID\n",
    "                (datetime(2023, 1, 1) + timedelta(days=random.randint(0, 364))).strftime('%Y-%m-%d'),  # DATE as string\n",
    "                random.choice([\"Electronics\", \"Clothing\", \"Books\", \"Home\", \"Sports\"]),  # PRODUCT_CATEGORY\n",
    "                random.choice([\"North\", \"South\", \"East\", \"West\"]),  # REGION\n",
    "                round(random.uniform(10, 1000), 2),  # SALES_AMOUNT\n",
    "                round(random.uniform(10, 1000), 2),  # ORDER_VALUE\n",
    "                random.choice([\"completed\", \"shipped\", \"pending\", \"cancelled\"])  # STATUS\n",
    "            ]\n",
    "            for i in range(1, num_rows + 1)\n",
    "        ]\n",
    "\n",
    "        # Define column names\n",
    "        columns = [\"ORDER_ID\", \"DATE\", \"PRODUCT_CATEGORY\", \"REGION\", \n",
    "                  \"SALES_AMOUNT\", \"ORDER_VALUE\", \"STATUS\"]\n",
    "        \n",
    "        # Create Snowpark DataFrame directly from lists\n",
    "        snowpark_df = session.create_dataframe(data, columns)\n",
    "        \n",
    "        # Write to table\n",
    "        snowpark_df.write.mode(\"overwrite\").save_as_table(table_name)\n",
    "        print(f\"Successfully created table '{table_name}' with {num_rows} rows\")\n",
    "        \n",
    "        # Show sample of the data\n",
    "        print(\"\\nSample of the created data:\")\n",
    "        return session.sql(f\"SELECT * FROM {table_name} LIMIT 10\").show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating fake data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created table 'ORDERS' with 10000 rows\n",
      "\n",
      "Sample of the created data:\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_ID\"  |\"DATE\"      |\"PRODUCT_CATEGORY\"  |\"REGION\"  |\"SALES_AMOUNT\"  |\"ORDER_VALUE\"  |\"STATUS\"   |\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "|1           |2023-08-29  |Books               |South     |741.9           |371.36         |pending    |\n",
      "|2           |2023-05-25  |Sports              |West      |723.71          |550.68         |cancelled  |\n",
      "|3           |2023-10-19  |Electronics         |North     |132.31          |160.01         |shipped    |\n",
      "|4           |2023-09-29  |Home                |South     |944.23          |408.82         |pending    |\n",
      "|5           |2023-07-10  |Sports              |South     |959.51          |872.47         |shipped    |\n",
      "|6           |2023-07-12  |Clothing            |South     |469.21          |971.15         |cancelled  |\n",
      "|7           |2023-10-28  |Books               |North     |968.17          |384.72         |cancelled  |\n",
      "|8           |2023-02-17  |Clothing            |South     |284.04          |651.0          |completed  |\n",
      "|9           |2023-10-24  |Home                |South     |884.72          |535.54         |completed  |\n",
      "|10          |2023-08-11  |Clothing            |North     |352.19          |363.73         |cancelled  |\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create full dataset\n",
    "create_fake_data(\n",
    "    session=session,\n",
    "    num_rows=10000,\n",
    "    table_name=\"ORDERS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_aggregation(session: Session, aggregation_request: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs data aggregation based on the provided aggregation request.\n",
    "\n",
    "    This function takes a Snowflake session and an aggregation request in JSON format,\n",
    "    applies the specified filters, groups the data, calculates the requested metrics,\n",
    "    and saves the results to a target table in Snowflake.\n",
    "\n",
    "    Parameters\n",
    "    session : snowflake.snowpark.Session\n",
    "        An active Snowflake session object.\n",
    "    aggregation_request : str\n",
    "        A JSON string containing the aggregation specifications. It should include:\n",
    "        - source_table: Name of the source table in Snowflake.\n",
    "        - target_table: Name of the table where results will be saved.\n",
    "        - group_by: List of columns to group by.\n",
    "        - metrics: List of metrics to calculate, each with a name, function, and column.\n",
    "        - filters: List of filters to apply to the data.\n",
    "\n",
    "    The aggregation_request should have the following structure:\n",
    "        - TODO: Think about fine grain control, but for now you can have \n",
    "                <database>.<schema>.<table> and if the caller has access\n",
    "                then it will work correctly.\n",
    "    {\n",
    "        \"source_table\": str,\n",
    "        \"target_table\": str,\n",
    "        \"group_by\": List[str],\n",
    "        \"metrics\": List[Dict[str, str]],\n",
    "        \"filters\": List[Dict[str, Union[str, List[str]]]]\n",
    "    }\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The function doesn't return a value, but it prints a confirmation message\n",
    "        and saves the results to the specified target table in Snowflake.\n",
    "    \"\"\"\n",
    "\n",
    "    request = json.loads(aggregation_request)\n",
    "    \n",
    "    df = session.table(request['source_table'])\n",
    "    for filter in request['filters']:\n",
    "        if filter['operator'] == 'between':\n",
    "            df = df.filter((col(filter['column']) >= filter['value'][0]) & \n",
    "                           (col(filter['column']) <= filter['value'][1]))\n",
    "        elif filter['operator'] == 'in':\n",
    "            df = df.filter(col(filter['column']).isin(filter['value']))\n",
    "\n",
    "    df = df.group_by(request['group_by'])\n",
    "\n",
    "    aggs = []\n",
    "    for metric in request['metrics']:\n",
    "        if metric['function'] == 'sum':\n",
    "            aggs.append(sum(col(metric['column'])).alias(metric['name']))\n",
    "        elif metric['function'] == 'avg':\n",
    "            aggs.append(avg(col(metric['column'])).alias(metric['name']))\n",
    "        elif metric['function'] == 'count':\n",
    "            aggs.append(count(col(metric['column'])).alias(metric['name']))\n",
    "    df = df.agg(*aggs).with_column('created', to_char(current_timestamp(), 'YYYY-MM-DD HH24:MI:SS'))\n",
    "    \n",
    "    df.write.mode(\"overwrite\").save_as_table(request['target_table'])\n",
    "    \n",
    "    return f\"Aggregation completed. Results saved to {request['target_table']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aggregation completed. Results saved to orders_aggregates'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregation_request = json.dumps({\n",
    "    \"request_id\": \"AGG_001\",\n",
    "    \"source_table\": \"orders\",\n",
    "    \"target_table\": \"orders_aggregates\",\n",
    "    \"group_by\": [\"PRODUCT_CATEGORY\", \"REGION\"],\n",
    "    \"metrics\": [\n",
    "        {\"name\": \"TOTAL_SALES\", \"function\": \"sum\", \"column\": \"SALES_AMOUNT\"},\n",
    "        {\"name\": \"AVERAGE_ORDER_VALUE\", \"function\": \"avg\", \"column\": \"ORDER_VALUE\"},\n",
    "        {\"name\": \"ORDER_COUNT\", \"function\": \"count\", \"column\": \"ORDER_ID\"}\n",
    "    ],\n",
    "    \"filters\": [\n",
    "        {\"column\": \"DATE\", \"operator\": \"between\", \"value\": [\"2023-01-01\", \"2023-12-31\"]},\n",
    "        {\"column\": \"STATUS\", \"operator\": \"in\", \"value\": [\"completed\", \"shipped\"]}\n",
    "    ],\n",
    "    \"version\": \"1.0.0\"\n",
    "})\n",
    "\n",
    "perform_aggregation(session, aggregation_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------\n",
      "|\"PRODUCT_CATEGORY\"  |\"REGION\"  |\"TOTAL_SALES\"  |\"AVERAGE_ORDER_VALUE\"  |\"ORDER_COUNT\"  |\"CREATED\"            |\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "|Electronics         |North     |121611.75      |502.2308764940239      |251            |2024-12-17 11:13:11  |\n",
      "|Sports              |South     |127182.67      |511.0550199203187      |251            |2024-12-17 11:13:11  |\n",
      "|Clothing            |South     |136309.18      |548.9966798418973      |253            |2024-12-17 11:13:11  |\n",
      "|Home                |South     |151382.34      |480.46474637681155     |276            |2024-12-17 11:13:11  |\n",
      "|Electronics         |East      |137977.52      |494.1362548262548      |259            |2024-12-17 11:13:11  |\n",
      "|Books               |West      |135392.85      |489.70191570881224     |261            |2024-12-17 11:13:11  |\n",
      "|Clothing            |West      |100747.99      |487.33851851851847     |216            |2024-12-17 11:13:11  |\n",
      "|Clothing            |East      |123848.65      |502.2717479674797      |246            |2024-12-17 11:13:11  |\n",
      "|Clothing            |North     |128602.42      |508.28915057915066     |259            |2024-12-17 11:13:11  |\n",
      "|Electronics         |South     |117874.55      |501.7482251082251      |231            |2024-12-17 11:13:11  |\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(f\"SELECT * FROM orders_aggregates\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
