{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomFunctions\n",
    "\n",
    "> Functions and Procedure For Snowflake. This is a simple repository that contains the functions and procedures that I have created to show the ease of managment and the power for snowflake CLI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Deployment Guide\n",
    "\n",
    "## 1. Installing Snowflake CLI\n",
    "\n",
    "[Link](https://docs.snowflake.com/en/CUSTOM_FUNCTIONSeloper-guide/snowflake-cli-v2/installation/installation)\n",
    "\n",
    "[Cheat Sheet](https://github.com/Snowflake-Labs/sf-cheatsheets/blob/main/snowflake-cli.md)\n",
    "\n",
    "To install the Snowflake CLI, follow the instructions provided in the official installation guide. This guide covers all the necessary steps to set up the CLI on your system.\n",
    "\n",
    "## 2. Configuring the CLI\n",
    "To use the Snowflake CLI, you need to create a connection configuration in the ~/.snowflake/config.toml file. This file allows you to specify connection details for various environments. Refer to the Snowflake CLI configuration documentation for a comprehensive guide.\n",
    "\n",
    "[Using Snowpark in Snowflake CLI](https://docs.snowflake.com/en/CUSTOM_FUNCTIONSeloper-guide/snowflake-cli-v2/snowpark/overview)\n",
    "\n",
    "## 3. Setting Up Your Database and Schema\n",
    "\n",
    "You can use the Snowsight UI or the Snowflake CLI to execute the following SQL commands. While the code example uses the ACCOUNTADMIN role for simplicity, you should use a role with the appropriate permissions in your environment.\n",
    "\n",
    "```sql\n",
    "ALTER SESSION SET query_tag = '{\"team\":\"Solutions\",\"name\":\"JeremyDemlow\", \"version\":0.1, \"attributes\":{\"medium\":\"setup\", \"source\":\"DATASCIENCE\", \"purpose\": \"setup\"}}';\n",
    "\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "CREATE ROLE DATA_SCIENTIST;\n",
    "\n",
    "USE ROLE SYSADMIN;\n",
    "GRANT CREATE DATABASE ON ACCOUNT TO ROLE DATA_SCIENTIST;\n",
    "GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE DATA_SCIENTIST;\n",
    "GRANT CREATE COMPUTE POOL ON ACCOUNT TO ROLE DATA_SCIENTIST;\n",
    "\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "GRANT CREATE INTEGRATION ON ACCOUNT TO ROLE DATA_SCIENTIST;\n",
    "GRANT MONITOR USAGE ON ACCOUNT TO ROLE DATA_SCIENTIST;\n",
    "GRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE DATA_SCIENTIST;\n",
    "GRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE DATA_SCIENTIST;\n",
    "GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE DATA_SCIENTIST;\n",
    "SET my_user_var = (SELECT  '\"' || CURRENT_USER() || '\"' );\n",
    "GRANT ROLE data_scientist TO USER identifier($my_user_var);\n",
    "\n",
    "USE ROLE DATA_SCIENTIST;\n",
    "\n",
    "CREATE OR REPLACE WAREHOUSE DS_WH_XS\n",
    "  WAREHOUSE_SIZE = XSMALL\n",
    "  AUTO_SUSPEND = 120\n",
    "  AUTO_RESUME = TRUE;\n",
    "\n",
    "CREATE DATABASE DATASCIENCE; \n",
    "CREATE SCHEMA DATASCIENCE;\n",
    "USE SCHEMA DATASCIENCE;\n",
    "```\n",
    "\n",
    "## 4. Initializing and Deploying the Snowpark Project\n",
    "\n",
    "### Creating a Boilerplate\n",
    "To set up a boilerplate Snowpark project, use the following command:\n",
    "\n",
    "```bash\n",
    "snow init custom_functions --template example_snowpark\n",
    "```\n",
    "> Note: The boilerplate structure can be customized. In this example, adjustments were made to rename the default 'app/' directory to 'DATASCIENCE/' in the snowflake.yml file. You can modify these settings based on your preferences. As I have done here to match the CUSTOM_FUNCTIONSelopmen style that I enjoy.\n",
    "\n",
    "### Adding Repo To Snowflake\n",
    "\n",
    "```bash\n",
    "snow connection set-default DATASCIENCE\n",
    "snow connection test\n",
    "snow git setup DATASCIENCE\n",
    "```\n",
    "\n",
    "\n",
    "### Building and Deploying the Project\n",
    "\n",
    "To build and deploy the Snowpark project, execute:\n",
    "\n",
    "```bash\n",
    "snow snowpark build; snow snowpark deploy --replace; rm DATASCIENCE.zip; rm dependencies.zip; rm dev.zip; rm requirements.snowflake.txt\n",
    "```\n",
    "The deployment output will list the created objects, such as procedures and functions.\n",
    "\n",
    "## 5. Creating and Testing a Table\n",
    "\n",
    "### Creating a Fake Orders Table\n",
    "Navigate to the CUSTOM_FUNCTIONS/CUSTOM_FUNCTIONSelopment.ipynb notebook and run the cells to create a fake orders table in the CUSTOM_FUNCTIONS schema. This setup is essential for testing and CUSTOM_FUNCTIONSelopment.\n",
    "\n",
    "> Note: Running the entire notebook will also perform an aggregation in the CUSTOM_FUNCTIONS schema. You can defer this step if you prefer to follow the next steps.\n",
    "\n",
    "### Testing Functions and Procedures\n",
    "To test the functions and procedures, use the following commands in Snowsight or the Snowflake CLI:\n",
    "\n",
    "```bash\n",
    "# Run Function\n",
    "snow sql -q \"SELECT DATASCIENCE.CUSTOM_FUNCTIONS.HELLO_FUNCTION('I would like a sandwich, please');\"\n",
    "\n",
    "# Run Procedures\n",
    "snow sql -f ./customfunctions/files/perform_aggregation_config.sql\n",
    "now sql -f ./customfunctions/files/perform_aggregation_config.sql -D SOURCE_TABLE=DATASCIENCE.CUSTOM_FUNCTIONS.ORDERS -D TARGET_TABLE=DATASCIENCE.CUSTOM_FUNCTIONS.ORDERS_AGGREGATES\n",
    "```\n",
    "\n",
    "**In Snowsight:**\n",
    "```sql\n",
    "CREATE SCHEMA IF NOT EXISTS DATASCIENCE.UAT;\n",
    "-- Using caller rights of the session, but using CUSTOM_FUNCTIONS data and putting the results in uat\n",
    "CALL DATASCIENCE.CUSTOM_FUNCTIONS.PERFORM_AGGREGATION(\n",
    "'{\n",
    "    \"request_id\": \"AGG_002\",\n",
    "    \"source_table\": \"DATASCIENCE.CUSTOM_FUNCTIONS.orders\",\n",
    "    \"target_table\": \"DATASCIENCE.uat.orders_aggregates\",\n",
    "    \"group_by\": [\"PRODUCT_CATEGORY\", \"REGION\"],\n",
    "    \"metrics\": [\n",
    "        {\"name\": \"TOTAL_SALES\", \"function\": \"sum\", \"column\": \"SALES_AMOUNT\"},\n",
    "        {\"name\": \"AVERAGE_ORDER_VALUE\", \"function\": \"avg\", \"column\": \"ORDER_VALUE\"},\n",
    "        {\"name\": \"ORDER_COUNT\", \"function\": \"count\", \"column\": \"ORDER_ID\"}\n",
    "    ],\n",
    "    \"filters\": [\n",
    "        {\"column\": \"DATE\", \"operator\": \"between\", \"value\": [\"2023-01-01\", \"2023-12-31\"]},\n",
    "        {\"column\": \"STATUS\", \"operator\": \"in\", \"value\": [\"completed\", \"shipped\"]}\n",
    "    ],\n",
    "    \"version\": \"1.0.0\"\n",
    "}'\n",
    ");\n",
    "\n",
    "# This is showing how you can use a file in your procedures or functions.\n",
    "CALL DATASCIENCE.CUSTOM_FUNCTIONS.HELLO_PROCEDURE('Developer we should expect our yaml to appear');\n",
    "```\n",
    "\n",
    "This guide provides a streamlined approach to setting up and deploying your Snowflake environment with Snowpark. Customize as necessary to fit your project's requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
